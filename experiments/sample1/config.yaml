data_path: data/first_three_claims.json
output_dir: experiments/sample1

# Model identifiers
argumenter_model: gpt2
overseer_model: gpt2
oracle_model: gpt2
oracle_enabled: true

# Filenames for each stage (defaults handled by pipeline)
prompts_file: prompts.json
args_file: arguments.json
eval_file: evaluation.json

# RL training parameters
rl_epochs: 1
rl_batch_size: 16
rl_output: argumenter_rl.pt
rl_learning_rate: 1e-05
rl_clip_range: 0.2
rl_max_new_tokens: 256
# Value head configuration for actorâ€“critic PPO
value_head_hidden_size: 768
value_head_layers: 1
# Critic warmup configuration
value_warmup_epochs: 1
critic_lr: 1e-4

# Stage toggles
enable_prompts: false
enable_generate: false
enable_eval: false
enable_rl: true
# PPO script arguments
dataset_name: experiments/sample1/arguments.json
dataset_config: ""
dataset_train_split: ""
per_device_train_batch_size: 16
gradient_accumulation_steps: 1
total_episodes: 1
model_name_or_path: gpt2
sft_model_path: gpt2
reward_model_path: gpt2
missing_eos_penalty: 1.0
